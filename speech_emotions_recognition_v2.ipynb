{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speech</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4795</th>\n",
       "      <td>./AudioWAV/1059_TAI_ANG_XX.wav</td>\n",
       "      <td>angry.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5985</th>\n",
       "      <td>./AudioWAV/1074_IEO_NEU_XX.wav</td>\n",
       "      <td>neutral.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>./AudioWAV/1004_ITS_ANG_XX.wav</td>\n",
       "      <td>angry.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3741</th>\n",
       "      <td>./AudioWAV/1046_TSI_ANG_XX.wav</td>\n",
       "      <td>angry.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>./AudioWAV/1003_MTI_NEU_XX.wav</td>\n",
       "      <td>neutral.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              speech        label\n",
       "4795  ./AudioWAV/1059_TAI_ANG_XX.wav    angry.wav\n",
       "5985  ./AudioWAV/1074_IEO_NEU_XX.wav  neutral.wav\n",
       "279   ./AudioWAV/1004_ITS_ANG_XX.wav    angry.wav\n",
       "3741  ./AudioWAV/1046_TSI_ANG_XX.wav    angry.wav\n",
       "219   ./AudioWAV/1003_MTI_NEU_XX.wav  neutral.wav"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths=[]\n",
    "labels=[]\n",
    "for filename in os.listdir('./AudioWAV'):\n",
    "    \n",
    "    paths.append('./AudioWAV/' + filename)\n",
    "    file = filename.split('.')[0]\n",
    "   \n",
    "    label = file.split('_')[2]\n",
    "    if label == 'ANG':\n",
    "        labels.append('angry.wav')\n",
    "    elif label == 'DIS':\n",
    "        labels.append('disgust.wav')\n",
    "    elif label == 'FEA':\n",
    "        labels.append('fear.wav')\n",
    "    elif label == 'HAP':\n",
    "        labels.append('happy.wav')\n",
    "    elif label == 'NEU':\n",
    "        labels.append('neutral.wav')\n",
    "    elif label == 'SAD':\n",
    "        labels.append('sad.wav')\n",
    "        \n",
    "\n",
    "df_cremad = pd.DataFrame({'speech':paths,'label':labels})\n",
    "df_cremad.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MFCC(filename):\n",
    "    y, sr = librosa.load(filename,duration=3,offset=0.5)\n",
    "    return np.mean(librosa.feature.mfcc(y=y,sr=sr,n_mfcc=40).T,axis=0)\n",
    "\n",
    "mfcc_cremad = df_cremad['speech'].apply(lambda x:MFCC(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7442, 40, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X =[x for x in mfcc_cremad]\n",
    "X =np.array(X)\n",
    "X.shape\n",
    "X =np.expand_dims(X,-1)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe=OneHotEncoder()\n",
    "y = ohe.fit_transform(df_cremad[['label']] )\n",
    "y = y.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7442, 40, 1), (7442, 6))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['angry.wav', 'disgust.wav', 'fear.wav', 'happy.wav', 'neutral.wav',\n",
       "       'sad.wav'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cremad['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# Definindo os modelos\n",
    "\n",
    "class SimpleDNN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(SimpleDNN, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "class LSTMAttention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.attention = nn.Linear(hidden_size, 1)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        attn_weights = torch.softmax(self.attention(out), dim=1)\n",
    "        out = torch.sum(attn_weights * out, dim=1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_channels, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(64, 32, kernel_size=3, padding=1)\n",
    "        self.fc_input_size = 32 * 1 * 1\n",
    "        self.fc1 = nn.Linear(self.fc_input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(-1, self.fc_input_size)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CNNAttention(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes):\n",
    "        super(CNNAttention, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_channels, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(64, 32, kernel_size=3, padding=1)\n",
    "        self.attention = nn.Linear(32, 1)\n",
    "        self.fc = nn.Linear(32, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        attn_weights = self.softmax(self.attention(x.permute(0, 2, 1))).squeeze(-1)\n",
    "        attn_weights = attn_weights.unsqueeze(-1)\n",
    "        x = torch.sum(attn_weights * x, dim=2)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "# Construindo e treinando os modelos\n",
    "\n",
    "input_size = X.shape[1:]\n",
    "num_classes = y.shape[1]\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Convertendo os dados para tensores PyTorch\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32).to(device)\n",
    "\n",
    "# Definindo o tamanho do lote\n",
    "batch_size = 32\n",
    "\n",
    "# Criando conjuntos de dados PyTorch\n",
    "dataset = torch.utils.data.TensorDataset(X_tensor, y_tensor)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Definindo tensor para o LSTM\n",
    "X_tensorLSTM = X_tensor.permute(0, 2, 1)\n",
    "\n",
    "# Criando conjuntos de dados PyTorch para LSTM\n",
    "datasetLSTM = torch.utils.data.TensorDataset(X_tensorLSTM, y_tensor)\n",
    "train_sizeLSTM = int(0.8 * len(datasetLSTM))\n",
    "test_sizeLSTM = len(datasetLSTM) - train_sizeLSTM\n",
    "train_datasetLSTM, test_datasetLSTM = torch.utils.data.random_split(datasetLSTM, [train_sizeLSTM, test_sizeLSTM])\n",
    "\n",
    "# DataLoader para o LSTM\n",
    "train_loaderLSTM = DataLoader(train_datasetLSTM, batch_size=batch_size, shuffle=True)\n",
    "test_loaderLSTM = DataLoader(test_datasetLSTM, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Convertendo os dados para tensores PyTorch do CNN\n",
    "X_tensorCNN = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "\n",
    "# Criando conjuntos de dados PyTorch do CNN\n",
    "datasetCNN = torch.utils.data.TensorDataset(X_tensorCNN, y_tensor)\n",
    "train_sizeCNN = int(0.8 * len(datasetCNN))\n",
    "test_sizeCNN = len(datasetCNN) - train_sizeCNN\n",
    "train_datasetCNN, test_datasetCNN = torch.utils.data.random_split(datasetCNN, [train_sizeCNN, test_sizeCNN])\n",
    "\n",
    "# DataLoader para o CNN\n",
    "train_loaderCNN = DataLoader(train_datasetCNN, batch_size=batch_size, shuffle=True)\n",
    "test_loaderCNN = DataLoader(test_datasetCNN, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Função para treinamento\n",
    "def train(model, train_loader, criterion, optimizer, num_epochs=200):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            #print(outputs.shape)\n",
    "            loss = criterion(outputs, torch.max(labels, 1)[1])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == torch.max(labels, 1)[1]).sum().item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        accuracy = correct / total\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Loss: 2.8810, Accuracy: 0.1776\n",
      "Epoch [2/200], Loss: 1.8046, Accuracy: 0.1757\n",
      "Epoch [3/200], Loss: 1.7951, Accuracy: 0.1833\n",
      "Epoch [4/200], Loss: 1.7724, Accuracy: 0.1970\n",
      "Epoch [5/200], Loss: 1.7304, Accuracy: 0.2261\n",
      "Epoch [6/200], Loss: 1.6900, Accuracy: 0.2501\n",
      "Epoch [7/200], Loss: 1.6880, Accuracy: 0.2530\n",
      "Epoch [8/200], Loss: 1.6693, Accuracy: 0.2681\n",
      "Epoch [9/200], Loss: 1.6616, Accuracy: 0.2911\n",
      "Epoch [10/200], Loss: 1.6404, Accuracy: 0.3047\n",
      "Epoch [11/200], Loss: 1.6376, Accuracy: 0.2948\n",
      "Epoch [12/200], Loss: 1.6146, Accuracy: 0.3010\n",
      "Epoch [13/200], Loss: 1.6175, Accuracy: 0.3067\n",
      "Epoch [14/200], Loss: 1.6093, Accuracy: 0.3072\n",
      "Epoch [15/200], Loss: 1.5924, Accuracy: 0.3208\n",
      "Epoch [16/200], Loss: 1.5913, Accuracy: 0.3163\n",
      "Epoch [17/200], Loss: 1.5835, Accuracy: 0.3229\n",
      "Epoch [18/200], Loss: 1.5723, Accuracy: 0.3303\n",
      "Epoch [19/200], Loss: 1.5711, Accuracy: 0.3329\n",
      "Epoch [20/200], Loss: 1.5815, Accuracy: 0.3190\n",
      "Epoch [21/200], Loss: 1.5716, Accuracy: 0.3271\n",
      "Epoch [22/200], Loss: 1.5636, Accuracy: 0.3343\n",
      "Epoch [23/200], Loss: 1.5592, Accuracy: 0.3392\n",
      "Epoch [24/200], Loss: 1.5485, Accuracy: 0.3346\n",
      "Epoch [25/200], Loss: 1.5563, Accuracy: 0.3381\n",
      "Epoch [26/200], Loss: 1.5520, Accuracy: 0.3395\n",
      "Epoch [27/200], Loss: 1.5405, Accuracy: 0.3460\n",
      "Epoch [28/200], Loss: 1.5373, Accuracy: 0.3455\n",
      "Epoch [29/200], Loss: 1.5366, Accuracy: 0.3472\n",
      "Epoch [30/200], Loss: 1.5291, Accuracy: 0.3546\n",
      "Epoch [31/200], Loss: 1.5421, Accuracy: 0.3494\n",
      "Epoch [32/200], Loss: 1.5264, Accuracy: 0.3459\n",
      "Epoch [33/200], Loss: 1.5429, Accuracy: 0.3447\n",
      "Epoch [34/200], Loss: 1.5437, Accuracy: 0.3484\n",
      "Epoch [35/200], Loss: 1.5325, Accuracy: 0.3514\n",
      "Epoch [36/200], Loss: 1.5234, Accuracy: 0.3628\n",
      "Epoch [37/200], Loss: 1.5225, Accuracy: 0.3509\n",
      "Epoch [38/200], Loss: 1.5133, Accuracy: 0.3644\n",
      "Epoch [39/200], Loss: 1.5146, Accuracy: 0.3563\n",
      "Epoch [40/200], Loss: 1.5310, Accuracy: 0.3531\n",
      "Epoch [41/200], Loss: 1.5084, Accuracy: 0.3561\n",
      "Epoch [42/200], Loss: 1.5167, Accuracy: 0.3637\n",
      "Epoch [43/200], Loss: 1.5091, Accuracy: 0.3597\n",
      "Epoch [44/200], Loss: 1.5276, Accuracy: 0.3534\n",
      "Epoch [45/200], Loss: 1.5127, Accuracy: 0.3565\n",
      "Epoch [46/200], Loss: 1.5012, Accuracy: 0.3583\n",
      "Epoch [47/200], Loss: 1.4994, Accuracy: 0.3635\n",
      "Epoch [48/200], Loss: 1.5070, Accuracy: 0.3630\n",
      "Epoch [49/200], Loss: 1.4992, Accuracy: 0.3729\n",
      "Epoch [50/200], Loss: 1.5071, Accuracy: 0.3704\n",
      "Epoch [51/200], Loss: 1.5055, Accuracy: 0.3667\n",
      "Epoch [52/200], Loss: 1.4916, Accuracy: 0.3768\n",
      "Epoch [53/200], Loss: 1.4909, Accuracy: 0.3701\n",
      "Epoch [54/200], Loss: 1.4914, Accuracy: 0.3684\n",
      "Epoch [55/200], Loss: 1.4817, Accuracy: 0.3726\n",
      "Epoch [56/200], Loss: 1.4909, Accuracy: 0.3649\n",
      "Epoch [57/200], Loss: 1.4933, Accuracy: 0.3712\n",
      "Epoch [58/200], Loss: 1.4901, Accuracy: 0.3684\n",
      "Epoch [59/200], Loss: 1.4904, Accuracy: 0.3645\n",
      "Epoch [60/200], Loss: 1.4828, Accuracy: 0.3738\n",
      "Epoch [61/200], Loss: 1.4831, Accuracy: 0.3818\n",
      "Epoch [62/200], Loss: 1.4805, Accuracy: 0.3706\n",
      "Epoch [63/200], Loss: 1.4834, Accuracy: 0.3758\n",
      "Epoch [64/200], Loss: 1.4776, Accuracy: 0.3716\n",
      "Epoch [65/200], Loss: 1.4873, Accuracy: 0.3820\n",
      "Epoch [66/200], Loss: 1.4892, Accuracy: 0.3689\n",
      "Epoch [67/200], Loss: 1.4799, Accuracy: 0.3867\n",
      "Epoch [68/200], Loss: 1.4701, Accuracy: 0.3838\n",
      "Epoch [69/200], Loss: 1.4800, Accuracy: 0.3882\n",
      "Epoch [70/200], Loss: 1.4951, Accuracy: 0.3785\n",
      "Epoch [71/200], Loss: 1.4878, Accuracy: 0.3790\n",
      "Epoch [72/200], Loss: 1.4774, Accuracy: 0.3848\n",
      "Epoch [73/200], Loss: 1.4779, Accuracy: 0.3736\n",
      "Epoch [74/200], Loss: 1.4887, Accuracy: 0.3738\n",
      "Epoch [75/200], Loss: 1.4804, Accuracy: 0.3756\n",
      "Epoch [76/200], Loss: 1.4831, Accuracy: 0.3746\n",
      "Epoch [77/200], Loss: 1.5124, Accuracy: 0.3783\n",
      "Epoch [78/200], Loss: 1.4766, Accuracy: 0.3733\n",
      "Epoch [79/200], Loss: 1.4766, Accuracy: 0.3805\n",
      "Epoch [80/200], Loss: 1.4780, Accuracy: 0.3788\n",
      "Epoch [81/200], Loss: 1.4718, Accuracy: 0.3823\n",
      "Epoch [82/200], Loss: 1.4809, Accuracy: 0.3764\n",
      "Epoch [83/200], Loss: 1.4728, Accuracy: 0.3838\n",
      "Epoch [84/200], Loss: 1.4641, Accuracy: 0.3870\n",
      "Epoch [85/200], Loss: 1.4756, Accuracy: 0.3874\n",
      "Epoch [86/200], Loss: 1.4607, Accuracy: 0.3823\n",
      "Epoch [87/200], Loss: 1.4652, Accuracy: 0.3874\n",
      "Epoch [88/200], Loss: 1.4810, Accuracy: 0.3827\n",
      "Epoch [89/200], Loss: 1.4781, Accuracy: 0.3812\n",
      "Epoch [90/200], Loss: 1.4556, Accuracy: 0.3884\n",
      "Epoch [91/200], Loss: 1.4703, Accuracy: 0.3771\n",
      "Epoch [92/200], Loss: 1.4661, Accuracy: 0.3907\n",
      "Epoch [93/200], Loss: 1.4686, Accuracy: 0.3904\n",
      "Epoch [94/200], Loss: 1.4600, Accuracy: 0.3803\n",
      "Epoch [95/200], Loss: 1.4618, Accuracy: 0.3833\n",
      "Epoch [96/200], Loss: 1.4643, Accuracy: 0.3887\n",
      "Epoch [97/200], Loss: 1.4546, Accuracy: 0.3887\n",
      "Epoch [98/200], Loss: 1.4529, Accuracy: 0.3944\n",
      "Epoch [99/200], Loss: 1.4608, Accuracy: 0.3879\n",
      "Epoch [100/200], Loss: 1.4591, Accuracy: 0.3879\n",
      "Epoch [101/200], Loss: 1.4702, Accuracy: 0.3906\n",
      "Epoch [102/200], Loss: 1.4484, Accuracy: 0.3867\n",
      "Epoch [103/200], Loss: 1.4542, Accuracy: 0.3931\n",
      "Epoch [104/200], Loss: 1.4559, Accuracy: 0.3882\n",
      "Epoch [105/200], Loss: 1.4541, Accuracy: 0.3884\n",
      "Epoch [106/200], Loss: 1.4536, Accuracy: 0.3932\n",
      "Epoch [107/200], Loss: 1.4491, Accuracy: 0.3912\n",
      "Epoch [108/200], Loss: 1.4543, Accuracy: 0.3870\n",
      "Epoch [109/200], Loss: 1.4527, Accuracy: 0.3922\n",
      "Epoch [110/200], Loss: 1.4371, Accuracy: 0.3983\n",
      "Epoch [111/200], Loss: 1.4539, Accuracy: 0.3926\n",
      "Epoch [112/200], Loss: 1.4485, Accuracy: 0.3946\n",
      "Epoch [113/200], Loss: 1.4431, Accuracy: 0.3946\n",
      "Epoch [114/200], Loss: 1.4497, Accuracy: 0.3885\n",
      "Epoch [115/200], Loss: 1.4517, Accuracy: 0.3852\n",
      "Epoch [116/200], Loss: 1.4416, Accuracy: 0.3922\n",
      "Epoch [117/200], Loss: 1.4417, Accuracy: 0.3946\n",
      "Epoch [118/200], Loss: 1.4476, Accuracy: 0.3980\n",
      "Epoch [119/200], Loss: 1.4441, Accuracy: 0.3934\n",
      "Epoch [120/200], Loss: 1.4546, Accuracy: 0.4065\n",
      "Epoch [121/200], Loss: 1.4600, Accuracy: 0.3926\n",
      "Epoch [122/200], Loss: 1.4599, Accuracy: 0.3885\n",
      "Epoch [123/200], Loss: 1.4554, Accuracy: 0.3855\n",
      "Epoch [124/200], Loss: 1.4417, Accuracy: 0.3986\n",
      "Epoch [125/200], Loss: 1.4484, Accuracy: 0.3912\n",
      "Epoch [126/200], Loss: 1.4528, Accuracy: 0.3938\n",
      "Epoch [127/200], Loss: 1.4490, Accuracy: 0.3902\n",
      "Epoch [128/200], Loss: 1.4509, Accuracy: 0.3890\n",
      "Epoch [129/200], Loss: 1.4521, Accuracy: 0.3897\n",
      "Epoch [130/200], Loss: 1.4462, Accuracy: 0.3983\n",
      "Epoch [131/200], Loss: 1.4493, Accuracy: 0.3985\n",
      "Epoch [132/200], Loss: 1.4382, Accuracy: 0.4027\n",
      "Epoch [133/200], Loss: 1.4360, Accuracy: 0.4022\n",
      "Epoch [134/200], Loss: 1.4412, Accuracy: 0.3968\n",
      "Epoch [135/200], Loss: 1.4456, Accuracy: 0.3919\n",
      "Epoch [136/200], Loss: 1.4540, Accuracy: 0.3958\n",
      "Epoch [137/200], Loss: 1.4557, Accuracy: 0.3766\n",
      "Epoch [138/200], Loss: 1.4395, Accuracy: 0.3848\n",
      "Epoch [139/200], Loss: 1.4576, Accuracy: 0.3859\n",
      "Epoch [140/200], Loss: 1.4472, Accuracy: 0.3907\n",
      "Epoch [141/200], Loss: 1.4365, Accuracy: 0.4048\n",
      "Epoch [142/200], Loss: 1.4389, Accuracy: 0.3985\n",
      "Epoch [143/200], Loss: 1.4425, Accuracy: 0.3929\n",
      "Epoch [144/200], Loss: 1.4424, Accuracy: 0.3953\n",
      "Epoch [145/200], Loss: 1.4371, Accuracy: 0.4060\n",
      "Epoch [146/200], Loss: 1.4456, Accuracy: 0.3963\n",
      "Epoch [147/200], Loss: 1.4371, Accuracy: 0.3882\n",
      "Epoch [148/200], Loss: 1.4589, Accuracy: 0.3911\n",
      "Epoch [149/200], Loss: 1.4472, Accuracy: 0.3988\n",
      "Epoch [150/200], Loss: 1.4409, Accuracy: 0.4025\n",
      "Epoch [151/200], Loss: 1.4515, Accuracy: 0.3902\n",
      "Epoch [152/200], Loss: 1.4663, Accuracy: 0.3776\n",
      "Epoch [153/200], Loss: 1.4653, Accuracy: 0.3781\n",
      "Epoch [154/200], Loss: 1.4703, Accuracy: 0.3763\n",
      "Epoch [155/200], Loss: 1.4454, Accuracy: 0.3865\n",
      "Epoch [156/200], Loss: 1.4552, Accuracy: 0.3820\n",
      "Epoch [157/200], Loss: 1.4450, Accuracy: 0.3793\n",
      "Epoch [158/200], Loss: 1.4433, Accuracy: 0.3859\n",
      "Epoch [159/200], Loss: 1.4534, Accuracy: 0.3803\n",
      "Epoch [160/200], Loss: 1.4614, Accuracy: 0.3810\n",
      "Epoch [161/200], Loss: 1.4528, Accuracy: 0.3806\n",
      "Epoch [162/200], Loss: 1.4451, Accuracy: 0.3885\n",
      "Epoch [163/200], Loss: 1.4481, Accuracy: 0.3817\n",
      "Epoch [164/200], Loss: 1.4517, Accuracy: 0.3860\n",
      "Epoch [165/200], Loss: 1.4551, Accuracy: 0.3788\n",
      "Epoch [166/200], Loss: 1.4409, Accuracy: 0.3815\n",
      "Epoch [167/200], Loss: 1.4482, Accuracy: 0.3921\n",
      "Epoch [168/200], Loss: 1.4507, Accuracy: 0.3798\n",
      "Epoch [169/200], Loss: 1.4476, Accuracy: 0.3780\n",
      "Epoch [170/200], Loss: 1.4415, Accuracy: 0.3901\n",
      "Epoch [171/200], Loss: 1.4476, Accuracy: 0.3763\n",
      "Epoch [172/200], Loss: 1.4382, Accuracy: 0.3995\n",
      "Epoch [173/200], Loss: 1.4369, Accuracy: 0.3870\n",
      "Epoch [174/200], Loss: 1.4425, Accuracy: 0.3901\n",
      "Epoch [175/200], Loss: 1.4298, Accuracy: 0.4016\n",
      "Epoch [176/200], Loss: 1.4460, Accuracy: 0.3795\n",
      "Epoch [177/200], Loss: 1.4464, Accuracy: 0.3867\n",
      "Epoch [178/200], Loss: 1.4513, Accuracy: 0.3783\n",
      "Epoch [179/200], Loss: 1.4451, Accuracy: 0.3864\n",
      "Epoch [180/200], Loss: 1.4455, Accuracy: 0.3859\n",
      "Epoch [181/200], Loss: 1.4409, Accuracy: 0.3958\n",
      "Epoch [182/200], Loss: 1.4358, Accuracy: 0.3993\n",
      "Epoch [183/200], Loss: 1.4517, Accuracy: 0.3850\n",
      "Epoch [184/200], Loss: 1.4435, Accuracy: 0.3951\n",
      "Epoch [185/200], Loss: 1.4271, Accuracy: 0.4000\n",
      "Epoch [186/200], Loss: 1.4397, Accuracy: 0.3852\n",
      "Epoch [187/200], Loss: 1.4350, Accuracy: 0.3944\n",
      "Epoch [188/200], Loss: 1.4426, Accuracy: 0.3894\n",
      "Epoch [189/200], Loss: 1.4310, Accuracy: 0.3909\n",
      "Epoch [190/200], Loss: 1.4431, Accuracy: 0.3902\n",
      "Epoch [191/200], Loss: 1.4410, Accuracy: 0.3904\n",
      "Epoch [192/200], Loss: 1.4433, Accuracy: 0.3827\n",
      "Epoch [193/200], Loss: 1.4397, Accuracy: 0.3902\n",
      "Epoch [194/200], Loss: 1.4194, Accuracy: 0.3969\n",
      "Epoch [195/200], Loss: 1.4213, Accuracy: 0.4005\n",
      "Epoch [196/200], Loss: 1.4299, Accuracy: 0.4016\n",
      "Epoch [197/200], Loss: 1.4307, Accuracy: 0.3926\n",
      "Epoch [198/200], Loss: 1.4244, Accuracy: 0.4028\n",
      "Epoch [199/200], Loss: 1.4321, Accuracy: 0.3854\n",
      "Epoch [200/200], Loss: 1.4191, Accuracy: 0.4006\n"
     ]
    }
   ],
   "source": [
    "# Standard Deep Neural Network\n",
    "sdnn_model = SimpleDNN(input_size[0], num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(sdnn_model.parameters(), lr=0.001)\n",
    "train(sdnn_model, train_loader, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Loss: 1.6698, Accuracy: 0.2894\n",
      "Epoch [2/200], Loss: 1.5579, Accuracy: 0.3388\n",
      "Epoch [3/200], Loss: 1.5366, Accuracy: 0.3477\n",
      "Epoch [4/200], Loss: 1.5211, Accuracy: 0.3670\n",
      "Epoch [5/200], Loss: 1.5068, Accuracy: 0.3684\n",
      "Epoch [6/200], Loss: 1.4958, Accuracy: 0.3751\n",
      "Epoch [7/200], Loss: 1.4861, Accuracy: 0.3845\n",
      "Epoch [8/200], Loss: 1.4710, Accuracy: 0.3931\n",
      "Epoch [9/200], Loss: 1.4665, Accuracy: 0.3902\n",
      "Epoch [10/200], Loss: 1.4707, Accuracy: 0.3964\n",
      "Epoch [11/200], Loss: 1.4391, Accuracy: 0.4052\n",
      "Epoch [12/200], Loss: 1.4307, Accuracy: 0.4111\n",
      "Epoch [13/200], Loss: 1.4188, Accuracy: 0.4211\n",
      "Epoch [14/200], Loss: 1.4187, Accuracy: 0.4136\n",
      "Epoch [15/200], Loss: 1.4064, Accuracy: 0.4208\n",
      "Epoch [16/200], Loss: 1.3895, Accuracy: 0.4284\n",
      "Epoch [17/200], Loss: 1.3807, Accuracy: 0.4272\n",
      "Epoch [18/200], Loss: 1.3846, Accuracy: 0.4373\n",
      "Epoch [19/200], Loss: 1.3750, Accuracy: 0.4391\n",
      "Epoch [20/200], Loss: 1.3606, Accuracy: 0.4430\n",
      "Epoch [21/200], Loss: 1.3525, Accuracy: 0.4448\n",
      "Epoch [22/200], Loss: 1.3568, Accuracy: 0.4468\n",
      "Epoch [23/200], Loss: 1.3438, Accuracy: 0.4504\n",
      "Epoch [24/200], Loss: 1.3336, Accuracy: 0.4552\n",
      "Epoch [25/200], Loss: 1.3453, Accuracy: 0.4462\n",
      "Epoch [26/200], Loss: 1.3601, Accuracy: 0.4480\n",
      "Epoch [27/200], Loss: 1.3553, Accuracy: 0.4487\n",
      "Epoch [28/200], Loss: 1.3358, Accuracy: 0.4564\n",
      "Epoch [29/200], Loss: 1.3251, Accuracy: 0.4655\n",
      "Epoch [30/200], Loss: 1.3269, Accuracy: 0.4628\n",
      "Epoch [31/200], Loss: 1.3163, Accuracy: 0.4618\n",
      "Epoch [32/200], Loss: 1.3269, Accuracy: 0.4556\n",
      "Epoch [33/200], Loss: 1.3229, Accuracy: 0.4636\n",
      "Epoch [34/200], Loss: 1.3223, Accuracy: 0.4635\n",
      "Epoch [35/200], Loss: 1.3078, Accuracy: 0.4614\n",
      "Epoch [36/200], Loss: 1.3126, Accuracy: 0.4650\n",
      "Epoch [37/200], Loss: 1.3031, Accuracy: 0.4761\n",
      "Epoch [38/200], Loss: 1.3094, Accuracy: 0.4646\n",
      "Epoch [39/200], Loss: 1.3045, Accuracy: 0.4740\n",
      "Epoch [40/200], Loss: 1.3182, Accuracy: 0.4683\n",
      "Epoch [41/200], Loss: 1.2986, Accuracy: 0.4729\n",
      "Epoch [42/200], Loss: 1.3027, Accuracy: 0.4772\n",
      "Epoch [43/200], Loss: 1.3157, Accuracy: 0.4643\n",
      "Epoch [44/200], Loss: 1.3020, Accuracy: 0.4720\n",
      "Epoch [45/200], Loss: 1.2983, Accuracy: 0.4742\n",
      "Epoch [46/200], Loss: 1.2929, Accuracy: 0.4761\n",
      "Epoch [47/200], Loss: 1.2922, Accuracy: 0.4762\n",
      "Epoch [48/200], Loss: 1.2907, Accuracy: 0.4806\n",
      "Epoch [49/200], Loss: 1.2853, Accuracy: 0.4798\n",
      "Epoch [50/200], Loss: 1.2757, Accuracy: 0.4841\n",
      "Epoch [51/200], Loss: 1.2720, Accuracy: 0.4838\n",
      "Epoch [52/200], Loss: 1.2746, Accuracy: 0.4786\n",
      "Epoch [53/200], Loss: 1.2766, Accuracy: 0.4850\n",
      "Epoch [54/200], Loss: 1.2679, Accuracy: 0.4890\n",
      "Epoch [55/200], Loss: 1.2605, Accuracy: 0.4937\n",
      "Epoch [56/200], Loss: 1.2470, Accuracy: 0.5003\n",
      "Epoch [57/200], Loss: 1.2690, Accuracy: 0.4929\n",
      "Epoch [58/200], Loss: 1.2584, Accuracy: 0.4934\n",
      "Epoch [59/200], Loss: 1.2774, Accuracy: 0.4791\n",
      "Epoch [60/200], Loss: 1.2584, Accuracy: 0.4947\n",
      "Epoch [61/200], Loss: 1.2461, Accuracy: 0.4996\n",
      "Epoch [62/200], Loss: 1.2438, Accuracy: 0.5008\n",
      "Epoch [63/200], Loss: 1.2334, Accuracy: 0.5019\n",
      "Epoch [64/200], Loss: 1.2295, Accuracy: 0.5033\n",
      "Epoch [65/200], Loss: 1.2397, Accuracy: 0.5038\n",
      "Epoch [66/200], Loss: 1.2385, Accuracy: 0.5023\n",
      "Epoch [67/200], Loss: 1.2384, Accuracy: 0.5060\n",
      "Epoch [68/200], Loss: 1.2526, Accuracy: 0.5006\n",
      "Epoch [69/200], Loss: 1.2309, Accuracy: 0.5070\n",
      "Epoch [70/200], Loss: 1.2268, Accuracy: 0.5065\n",
      "Epoch [71/200], Loss: 1.2209, Accuracy: 0.5093\n",
      "Epoch [72/200], Loss: 1.2156, Accuracy: 0.5137\n",
      "Epoch [73/200], Loss: 1.2201, Accuracy: 0.5078\n",
      "Epoch [74/200], Loss: 1.2238, Accuracy: 0.5100\n",
      "Epoch [75/200], Loss: 1.2135, Accuracy: 0.5140\n",
      "Epoch [76/200], Loss: 1.2090, Accuracy: 0.5196\n",
      "Epoch [77/200], Loss: 1.2147, Accuracy: 0.5145\n",
      "Epoch [78/200], Loss: 1.2220, Accuracy: 0.5120\n",
      "Epoch [79/200], Loss: 1.2159, Accuracy: 0.5127\n",
      "Epoch [80/200], Loss: 1.2191, Accuracy: 0.5164\n",
      "Epoch [81/200], Loss: 1.2358, Accuracy: 0.5056\n",
      "Epoch [82/200], Loss: 1.2294, Accuracy: 0.5076\n",
      "Epoch [83/200], Loss: 1.2071, Accuracy: 0.5130\n",
      "Epoch [84/200], Loss: 1.2050, Accuracy: 0.5150\n",
      "Epoch [85/200], Loss: 1.1995, Accuracy: 0.5159\n",
      "Epoch [86/200], Loss: 1.2222, Accuracy: 0.5100\n",
      "Epoch [87/200], Loss: 1.2164, Accuracy: 0.5120\n",
      "Epoch [88/200], Loss: 1.2056, Accuracy: 0.5196\n",
      "Epoch [89/200], Loss: 1.2061, Accuracy: 0.5191\n",
      "Epoch [90/200], Loss: 1.1983, Accuracy: 0.5157\n",
      "Epoch [91/200], Loss: 1.1985, Accuracy: 0.5192\n",
      "Epoch [92/200], Loss: 1.1876, Accuracy: 0.5291\n",
      "Epoch [93/200], Loss: 1.1933, Accuracy: 0.5256\n",
      "Epoch [94/200], Loss: 1.1966, Accuracy: 0.5251\n",
      "Epoch [95/200], Loss: 1.1834, Accuracy: 0.5310\n",
      "Epoch [96/200], Loss: 1.2070, Accuracy: 0.5174\n",
      "Epoch [97/200], Loss: 1.2066, Accuracy: 0.5174\n",
      "Epoch [98/200], Loss: 1.1890, Accuracy: 0.5218\n",
      "Epoch [99/200], Loss: 1.1832, Accuracy: 0.5302\n",
      "Epoch [100/200], Loss: 1.2001, Accuracy: 0.5179\n",
      "Epoch [101/200], Loss: 1.1694, Accuracy: 0.5327\n",
      "Epoch [102/200], Loss: 1.1705, Accuracy: 0.5283\n",
      "Epoch [103/200], Loss: 1.1694, Accuracy: 0.5402\n",
      "Epoch [104/200], Loss: 1.1730, Accuracy: 0.5332\n",
      "Epoch [105/200], Loss: 1.1662, Accuracy: 0.5310\n",
      "Epoch [106/200], Loss: 1.1690, Accuracy: 0.5305\n",
      "Epoch [107/200], Loss: 1.1705, Accuracy: 0.5320\n",
      "Epoch [108/200], Loss: 1.1719, Accuracy: 0.5369\n",
      "Epoch [109/200], Loss: 1.1576, Accuracy: 0.5382\n",
      "Epoch [110/200], Loss: 1.1497, Accuracy: 0.5424\n",
      "Epoch [111/200], Loss: 1.1634, Accuracy: 0.5337\n",
      "Epoch [112/200], Loss: 1.1587, Accuracy: 0.5379\n",
      "Epoch [113/200], Loss: 1.1536, Accuracy: 0.5412\n",
      "Epoch [114/200], Loss: 1.1485, Accuracy: 0.5439\n",
      "Epoch [115/200], Loss: 1.1423, Accuracy: 0.5422\n",
      "Epoch [116/200], Loss: 1.1412, Accuracy: 0.5433\n",
      "Epoch [117/200], Loss: 1.1437, Accuracy: 0.5475\n",
      "Epoch [118/200], Loss: 1.1549, Accuracy: 0.5424\n",
      "Epoch [119/200], Loss: 1.1645, Accuracy: 0.5302\n",
      "Epoch [120/200], Loss: 1.1510, Accuracy: 0.5402\n",
      "Epoch [121/200], Loss: 1.1497, Accuracy: 0.5392\n",
      "Epoch [122/200], Loss: 1.1527, Accuracy: 0.5451\n",
      "Epoch [123/200], Loss: 1.1443, Accuracy: 0.5404\n",
      "Epoch [124/200], Loss: 1.1502, Accuracy: 0.5355\n",
      "Epoch [125/200], Loss: 1.1457, Accuracy: 0.5466\n",
      "Epoch [126/200], Loss: 1.1356, Accuracy: 0.5481\n",
      "Epoch [127/200], Loss: 1.1394, Accuracy: 0.5453\n",
      "Epoch [128/200], Loss: 1.1298, Accuracy: 0.5483\n",
      "Epoch [129/200], Loss: 1.1331, Accuracy: 0.5525\n",
      "Epoch [130/200], Loss: 1.1410, Accuracy: 0.5421\n",
      "Epoch [131/200], Loss: 1.1373, Accuracy: 0.5444\n",
      "Epoch [132/200], Loss: 1.1297, Accuracy: 0.5508\n",
      "Epoch [133/200], Loss: 1.1457, Accuracy: 0.5488\n",
      "Epoch [134/200], Loss: 1.1554, Accuracy: 0.5463\n",
      "Epoch [135/200], Loss: 1.1412, Accuracy: 0.5513\n",
      "Epoch [136/200], Loss: 1.1313, Accuracy: 0.5518\n",
      "Epoch [137/200], Loss: 1.1233, Accuracy: 0.5513\n",
      "Epoch [138/200], Loss: 1.1275, Accuracy: 0.5513\n",
      "Epoch [139/200], Loss: 1.1276, Accuracy: 0.5575\n",
      "Epoch [140/200], Loss: 1.1327, Accuracy: 0.5548\n",
      "Epoch [141/200], Loss: 1.1452, Accuracy: 0.5506\n",
      "Epoch [142/200], Loss: 1.1246, Accuracy: 0.5559\n",
      "Epoch [143/200], Loss: 1.1134, Accuracy: 0.5572\n",
      "Epoch [144/200], Loss: 1.1027, Accuracy: 0.5631\n",
      "Epoch [145/200], Loss: 1.1128, Accuracy: 0.5604\n",
      "Epoch [146/200], Loss: 1.1198, Accuracy: 0.5614\n",
      "Epoch [147/200], Loss: 1.1126, Accuracy: 0.5532\n",
      "Epoch [148/200], Loss: 1.0982, Accuracy: 0.5666\n",
      "Epoch [149/200], Loss: 1.1152, Accuracy: 0.5654\n",
      "Epoch [150/200], Loss: 1.1230, Accuracy: 0.5538\n",
      "Epoch [151/200], Loss: 1.1204, Accuracy: 0.5595\n",
      "Epoch [152/200], Loss: 1.1516, Accuracy: 0.5357\n",
      "Epoch [153/200], Loss: 1.1479, Accuracy: 0.5441\n",
      "Epoch [154/200], Loss: 1.1141, Accuracy: 0.5631\n",
      "Epoch [155/200], Loss: 1.1126, Accuracy: 0.5622\n",
      "Epoch [156/200], Loss: 1.1217, Accuracy: 0.5636\n",
      "Epoch [157/200], Loss: 1.1057, Accuracy: 0.5668\n",
      "Epoch [158/200], Loss: 1.1018, Accuracy: 0.5668\n",
      "Epoch [159/200], Loss: 1.1130, Accuracy: 0.5666\n",
      "Epoch [160/200], Loss: 1.0998, Accuracy: 0.5695\n",
      "Epoch [161/200], Loss: 1.1035, Accuracy: 0.5592\n",
      "Epoch [162/200], Loss: 1.0977, Accuracy: 0.5622\n",
      "Epoch [163/200], Loss: 1.1120, Accuracy: 0.5626\n",
      "Epoch [164/200], Loss: 1.0876, Accuracy: 0.5705\n",
      "Epoch [165/200], Loss: 1.0873, Accuracy: 0.5698\n",
      "Epoch [166/200], Loss: 1.0791, Accuracy: 0.5708\n",
      "Epoch [167/200], Loss: 1.0963, Accuracy: 0.5619\n",
      "Epoch [168/200], Loss: 1.0890, Accuracy: 0.5758\n",
      "Epoch [169/200], Loss: 1.0908, Accuracy: 0.5760\n",
      "Epoch [170/200], Loss: 1.1173, Accuracy: 0.5599\n",
      "Epoch [171/200], Loss: 1.0695, Accuracy: 0.5787\n",
      "Epoch [172/200], Loss: 1.0834, Accuracy: 0.5721\n",
      "Epoch [173/200], Loss: 1.0823, Accuracy: 0.5770\n",
      "Epoch [174/200], Loss: 1.0720, Accuracy: 0.5750\n",
      "Epoch [175/200], Loss: 1.0687, Accuracy: 0.5784\n",
      "Epoch [176/200], Loss: 1.0778, Accuracy: 0.5763\n",
      "Epoch [177/200], Loss: 1.0991, Accuracy: 0.5631\n",
      "Epoch [178/200], Loss: 1.0682, Accuracy: 0.5779\n",
      "Epoch [179/200], Loss: 1.0730, Accuracy: 0.5814\n",
      "Epoch [180/200], Loss: 1.0920, Accuracy: 0.5668\n",
      "Epoch [181/200], Loss: 1.0764, Accuracy: 0.5721\n",
      "Epoch [182/200], Loss: 1.0986, Accuracy: 0.5654\n",
      "Epoch [183/200], Loss: 1.0810, Accuracy: 0.5812\n",
      "Epoch [184/200], Loss: 1.0586, Accuracy: 0.5834\n",
      "Epoch [185/200], Loss: 1.0594, Accuracy: 0.5785\n",
      "Epoch [186/200], Loss: 1.0554, Accuracy: 0.5831\n",
      "Epoch [187/200], Loss: 1.0638, Accuracy: 0.5765\n",
      "Epoch [188/200], Loss: 1.0600, Accuracy: 0.5784\n",
      "Epoch [189/200], Loss: 1.0736, Accuracy: 0.5747\n",
      "Epoch [190/200], Loss: 1.0514, Accuracy: 0.5809\n",
      "Epoch [191/200], Loss: 1.0711, Accuracy: 0.5821\n",
      "Epoch [192/200], Loss: 1.1147, Accuracy: 0.5619\n",
      "Epoch [193/200], Loss: 1.0633, Accuracy: 0.5842\n",
      "Epoch [194/200], Loss: 1.0720, Accuracy: 0.5817\n",
      "Epoch [195/200], Loss: 1.0517, Accuracy: 0.5831\n",
      "Epoch [196/200], Loss: 1.0516, Accuracy: 0.5895\n",
      "Epoch [197/200], Loss: 1.0497, Accuracy: 0.5817\n",
      "Epoch [198/200], Loss: 1.0397, Accuracy: 0.5931\n",
      "Epoch [199/200], Loss: 1.0538, Accuracy: 0.5790\n",
      "Epoch [200/200], Loss: 1.0412, Accuracy: 0.5908\n"
     ]
    }
   ],
   "source": [
    "# LSTM\n",
    "lstm_model = LSTMModel(input_size[0], hidden_size, num_layers, num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=0.001)\n",
    "train(lstm_model, train_loaderLSTM, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Loss: 1.6588, Accuracy: 0.2950\n",
      "Epoch [2/200], Loss: 1.5541, Accuracy: 0.3496\n",
      "Epoch [3/200], Loss: 1.5363, Accuracy: 0.3610\n",
      "Epoch [4/200], Loss: 1.5121, Accuracy: 0.3726\n",
      "Epoch [5/200], Loss: 1.4958, Accuracy: 0.3758\n",
      "Epoch [6/200], Loss: 1.4707, Accuracy: 0.3973\n",
      "Epoch [7/200], Loss: 1.4537, Accuracy: 0.4000\n",
      "Epoch [8/200], Loss: 1.4326, Accuracy: 0.4047\n",
      "Epoch [9/200], Loss: 1.4178, Accuracy: 0.4151\n",
      "Epoch [10/200], Loss: 1.4195, Accuracy: 0.4191\n",
      "Epoch [11/200], Loss: 1.4030, Accuracy: 0.4226\n",
      "Epoch [12/200], Loss: 1.3971, Accuracy: 0.4347\n",
      "Epoch [13/200], Loss: 1.3985, Accuracy: 0.4263\n",
      "Epoch [14/200], Loss: 1.3902, Accuracy: 0.4295\n",
      "Epoch [15/200], Loss: 1.3702, Accuracy: 0.4376\n",
      "Epoch [16/200], Loss: 1.3644, Accuracy: 0.4497\n",
      "Epoch [17/200], Loss: 1.3698, Accuracy: 0.4418\n",
      "Epoch [18/200], Loss: 1.3637, Accuracy: 0.4441\n",
      "Epoch [19/200], Loss: 1.3506, Accuracy: 0.4460\n",
      "Epoch [20/200], Loss: 1.3495, Accuracy: 0.4448\n",
      "Epoch [21/200], Loss: 1.3547, Accuracy: 0.4527\n",
      "Epoch [22/200], Loss: 1.3564, Accuracy: 0.4448\n",
      "Epoch [23/200], Loss: 1.3453, Accuracy: 0.4457\n",
      "Epoch [24/200], Loss: 1.3368, Accuracy: 0.4611\n",
      "Epoch [25/200], Loss: 1.3438, Accuracy: 0.4499\n",
      "Epoch [26/200], Loss: 1.3354, Accuracy: 0.4541\n",
      "Epoch [27/200], Loss: 1.3236, Accuracy: 0.4601\n",
      "Epoch [28/200], Loss: 1.3259, Accuracy: 0.4584\n",
      "Epoch [29/200], Loss: 1.3160, Accuracy: 0.4650\n",
      "Epoch [30/200], Loss: 1.3066, Accuracy: 0.4668\n",
      "Epoch [31/200], Loss: 1.3127, Accuracy: 0.4670\n",
      "Epoch [32/200], Loss: 1.3084, Accuracy: 0.4678\n",
      "Epoch [33/200], Loss: 1.3057, Accuracy: 0.4734\n",
      "Epoch [34/200], Loss: 1.2993, Accuracy: 0.4757\n",
      "Epoch [35/200], Loss: 1.3058, Accuracy: 0.4685\n",
      "Epoch [36/200], Loss: 1.3122, Accuracy: 0.4640\n",
      "Epoch [37/200], Loss: 1.3012, Accuracy: 0.4751\n",
      "Epoch [38/200], Loss: 1.3250, Accuracy: 0.4611\n",
      "Epoch [39/200], Loss: 1.3082, Accuracy: 0.4722\n",
      "Epoch [40/200], Loss: 1.2830, Accuracy: 0.4729\n",
      "Epoch [41/200], Loss: 1.2775, Accuracy: 0.4719\n",
      "Epoch [42/200], Loss: 1.2687, Accuracy: 0.4843\n",
      "Epoch [43/200], Loss: 1.2855, Accuracy: 0.4803\n",
      "Epoch [44/200], Loss: 1.2764, Accuracy: 0.4828\n",
      "Epoch [45/200], Loss: 1.2724, Accuracy: 0.4814\n",
      "Epoch [46/200], Loss: 1.2640, Accuracy: 0.4856\n",
      "Epoch [47/200], Loss: 1.2669, Accuracy: 0.4927\n",
      "Epoch [48/200], Loss: 1.2796, Accuracy: 0.4851\n",
      "Epoch [49/200], Loss: 1.2729, Accuracy: 0.4823\n",
      "Epoch [50/200], Loss: 1.2513, Accuracy: 0.4922\n",
      "Epoch [51/200], Loss: 1.2609, Accuracy: 0.4897\n",
      "Epoch [52/200], Loss: 1.2477, Accuracy: 0.4885\n",
      "Epoch [53/200], Loss: 1.2616, Accuracy: 0.4850\n",
      "Epoch [54/200], Loss: 1.2858, Accuracy: 0.4766\n",
      "Epoch [55/200], Loss: 1.2688, Accuracy: 0.4885\n",
      "Epoch [56/200], Loss: 1.2671, Accuracy: 0.4919\n",
      "Epoch [57/200], Loss: 1.2588, Accuracy: 0.4893\n",
      "Epoch [58/200], Loss: 1.2473, Accuracy: 0.4925\n",
      "Epoch [59/200], Loss: 1.2431, Accuracy: 0.5006\n",
      "Epoch [60/200], Loss: 1.2415, Accuracy: 0.4982\n",
      "Epoch [61/200], Loss: 1.2289, Accuracy: 0.4979\n",
      "Epoch [62/200], Loss: 1.2316, Accuracy: 0.4954\n",
      "Epoch [63/200], Loss: 1.2399, Accuracy: 0.5021\n",
      "Epoch [64/200], Loss: 1.2498, Accuracy: 0.4917\n",
      "Epoch [65/200], Loss: 1.2434, Accuracy: 0.4977\n",
      "Epoch [66/200], Loss: 1.2175, Accuracy: 0.5048\n",
      "Epoch [67/200], Loss: 1.2417, Accuracy: 0.5045\n",
      "Epoch [68/200], Loss: 1.2255, Accuracy: 0.5001\n",
      "Epoch [69/200], Loss: 1.2334, Accuracy: 0.5018\n",
      "Epoch [70/200], Loss: 1.2173, Accuracy: 0.5058\n",
      "Epoch [71/200], Loss: 1.2196, Accuracy: 0.5075\n",
      "Epoch [72/200], Loss: 1.2148, Accuracy: 0.5100\n",
      "Epoch [73/200], Loss: 1.2235, Accuracy: 0.5061\n",
      "Epoch [74/200], Loss: 1.2188, Accuracy: 0.5024\n",
      "Epoch [75/200], Loss: 1.2036, Accuracy: 0.5118\n",
      "Epoch [76/200], Loss: 1.2010, Accuracy: 0.5092\n",
      "Epoch [77/200], Loss: 1.1948, Accuracy: 0.5191\n",
      "Epoch [78/200], Loss: 1.2067, Accuracy: 0.5066\n",
      "Epoch [79/200], Loss: 1.2057, Accuracy: 0.5164\n",
      "Epoch [80/200], Loss: 1.2086, Accuracy: 0.5055\n",
      "Epoch [81/200], Loss: 1.1915, Accuracy: 0.5120\n",
      "Epoch [82/200], Loss: 1.2044, Accuracy: 0.5122\n",
      "Epoch [83/200], Loss: 1.2066, Accuracy: 0.5098\n",
      "Epoch [84/200], Loss: 1.2113, Accuracy: 0.5088\n",
      "Epoch [85/200], Loss: 1.2109, Accuracy: 0.5092\n",
      "Epoch [86/200], Loss: 1.2565, Accuracy: 0.4882\n",
      "Epoch [87/200], Loss: 1.1963, Accuracy: 0.5174\n",
      "Epoch [88/200], Loss: 1.1916, Accuracy: 0.5147\n",
      "Epoch [89/200], Loss: 1.1892, Accuracy: 0.5191\n",
      "Epoch [90/200], Loss: 1.1791, Accuracy: 0.5181\n",
      "Epoch [91/200], Loss: 1.1840, Accuracy: 0.5201\n",
      "Epoch [92/200], Loss: 1.1793, Accuracy: 0.5219\n",
      "Epoch [93/200], Loss: 1.1966, Accuracy: 0.5179\n",
      "Epoch [94/200], Loss: 1.1665, Accuracy: 0.5224\n",
      "Epoch [95/200], Loss: 1.1774, Accuracy: 0.5197\n",
      "Epoch [96/200], Loss: 1.1823, Accuracy: 0.5209\n",
      "Epoch [97/200], Loss: 1.2124, Accuracy: 0.5092\n",
      "Epoch [98/200], Loss: 1.1729, Accuracy: 0.5249\n",
      "Epoch [99/200], Loss: 1.1739, Accuracy: 0.5226\n",
      "Epoch [100/200], Loss: 1.1711, Accuracy: 0.5229\n",
      "Epoch [101/200], Loss: 1.1799, Accuracy: 0.5249\n",
      "Epoch [102/200], Loss: 1.1868, Accuracy: 0.5194\n",
      "Epoch [103/200], Loss: 1.1711, Accuracy: 0.5228\n",
      "Epoch [104/200], Loss: 1.1669, Accuracy: 0.5223\n",
      "Epoch [105/200], Loss: 1.1837, Accuracy: 0.5244\n",
      "Epoch [106/200], Loss: 1.1872, Accuracy: 0.5221\n",
      "Epoch [107/200], Loss: 1.2118, Accuracy: 0.5008\n",
      "Epoch [108/200], Loss: 1.2013, Accuracy: 0.5159\n",
      "Epoch [109/200], Loss: 1.1770, Accuracy: 0.5248\n",
      "Epoch [110/200], Loss: 1.1882, Accuracy: 0.5085\n",
      "Epoch [111/200], Loss: 1.1556, Accuracy: 0.5317\n",
      "Epoch [112/200], Loss: 1.1478, Accuracy: 0.5379\n",
      "Epoch [113/200], Loss: 1.1603, Accuracy: 0.5349\n",
      "Epoch [114/200], Loss: 1.2136, Accuracy: 0.5117\n",
      "Epoch [115/200], Loss: 1.1845, Accuracy: 0.5236\n",
      "Epoch [116/200], Loss: 1.2049, Accuracy: 0.5081\n",
      "Epoch [117/200], Loss: 1.1902, Accuracy: 0.5113\n",
      "Epoch [118/200], Loss: 1.1693, Accuracy: 0.5260\n",
      "Epoch [119/200], Loss: 1.1650, Accuracy: 0.5407\n",
      "Epoch [120/200], Loss: 1.1749, Accuracy: 0.5202\n",
      "Epoch [121/200], Loss: 1.1512, Accuracy: 0.5387\n",
      "Epoch [122/200], Loss: 1.1416, Accuracy: 0.5352\n",
      "Epoch [123/200], Loss: 1.1474, Accuracy: 0.5379\n",
      "Epoch [124/200], Loss: 1.1331, Accuracy: 0.5404\n",
      "Epoch [125/200], Loss: 1.1304, Accuracy: 0.5384\n",
      "Epoch [126/200], Loss: 1.1312, Accuracy: 0.5426\n",
      "Epoch [127/200], Loss: 1.1318, Accuracy: 0.5419\n",
      "Epoch [128/200], Loss: 1.1157, Accuracy: 0.5453\n",
      "Epoch [129/200], Loss: 1.1198, Accuracy: 0.5448\n",
      "Epoch [130/200], Loss: 1.1273, Accuracy: 0.5441\n",
      "Epoch [131/200], Loss: 1.1224, Accuracy: 0.5426\n",
      "Epoch [132/200], Loss: 1.1351, Accuracy: 0.5421\n",
      "Epoch [133/200], Loss: 1.1083, Accuracy: 0.5562\n",
      "Epoch [134/200], Loss: 1.1255, Accuracy: 0.5476\n",
      "Epoch [135/200], Loss: 1.1227, Accuracy: 0.5459\n",
      "Epoch [136/200], Loss: 1.1073, Accuracy: 0.5565\n",
      "Epoch [137/200], Loss: 1.1147, Accuracy: 0.5552\n",
      "Epoch [138/200], Loss: 1.1205, Accuracy: 0.5473\n",
      "Epoch [139/200], Loss: 1.1624, Accuracy: 0.5291\n",
      "Epoch [140/200], Loss: 1.1244, Accuracy: 0.5543\n",
      "Epoch [141/200], Loss: 1.1056, Accuracy: 0.5525\n",
      "Epoch [142/200], Loss: 1.1060, Accuracy: 0.5548\n",
      "Epoch [143/200], Loss: 1.1104, Accuracy: 0.5513\n",
      "Epoch [144/200], Loss: 1.1326, Accuracy: 0.5424\n",
      "Epoch [145/200], Loss: 1.1172, Accuracy: 0.5429\n",
      "Epoch [146/200], Loss: 1.1028, Accuracy: 0.5466\n",
      "Epoch [147/200], Loss: 1.0939, Accuracy: 0.5572\n",
      "Epoch [148/200], Loss: 1.0978, Accuracy: 0.5564\n",
      "Epoch [149/200], Loss: 1.1043, Accuracy: 0.5569\n",
      "Epoch [150/200], Loss: 1.1092, Accuracy: 0.5520\n",
      "Epoch [151/200], Loss: 1.0980, Accuracy: 0.5575\n",
      "Epoch [152/200], Loss: 1.0951, Accuracy: 0.5575\n",
      "Epoch [153/200], Loss: 1.0924, Accuracy: 0.5609\n",
      "Epoch [154/200], Loss: 1.0855, Accuracy: 0.5624\n",
      "Epoch [155/200], Loss: 1.0772, Accuracy: 0.5700\n",
      "Epoch [156/200], Loss: 1.1250, Accuracy: 0.5493\n",
      "Epoch [157/200], Loss: 1.0934, Accuracy: 0.5585\n",
      "Epoch [158/200], Loss: 1.0785, Accuracy: 0.5646\n",
      "Epoch [159/200], Loss: 1.0816, Accuracy: 0.5651\n",
      "Epoch [160/200], Loss: 1.0883, Accuracy: 0.5601\n",
      "Epoch [161/200], Loss: 1.0874, Accuracy: 0.5622\n",
      "Epoch [162/200], Loss: 1.0865, Accuracy: 0.5622\n",
      "Epoch [163/200], Loss: 1.0714, Accuracy: 0.5701\n",
      "Epoch [164/200], Loss: 1.0760, Accuracy: 0.5691\n",
      "Epoch [165/200], Loss: 1.0812, Accuracy: 0.5627\n",
      "Epoch [166/200], Loss: 1.0746, Accuracy: 0.5740\n",
      "Epoch [167/200], Loss: 1.0922, Accuracy: 0.5725\n",
      "Epoch [168/200], Loss: 1.1374, Accuracy: 0.5424\n",
      "Epoch [169/200], Loss: 1.1045, Accuracy: 0.5552\n",
      "Epoch [170/200], Loss: 1.0586, Accuracy: 0.5775\n",
      "Epoch [171/200], Loss: 1.0739, Accuracy: 0.5691\n",
      "Epoch [172/200], Loss: 1.0674, Accuracy: 0.5755\n",
      "Epoch [173/200], Loss: 1.0640, Accuracy: 0.5752\n",
      "Epoch [174/200], Loss: 1.0821, Accuracy: 0.5691\n",
      "Epoch [175/200], Loss: 1.0683, Accuracy: 0.5718\n",
      "Epoch [176/200], Loss: 1.0596, Accuracy: 0.5824\n",
      "Epoch [177/200], Loss: 1.0621, Accuracy: 0.5769\n",
      "Epoch [178/200], Loss: 1.0534, Accuracy: 0.5758\n",
      "Epoch [179/200], Loss: 1.0611, Accuracy: 0.5770\n",
      "Epoch [180/200], Loss: 1.0534, Accuracy: 0.5762\n",
      "Epoch [181/200], Loss: 1.0527, Accuracy: 0.5772\n",
      "Epoch [182/200], Loss: 1.0378, Accuracy: 0.5824\n",
      "Epoch [183/200], Loss: 1.0447, Accuracy: 0.5772\n",
      "Epoch [184/200], Loss: 1.0421, Accuracy: 0.5824\n",
      "Epoch [185/200], Loss: 1.0572, Accuracy: 0.5780\n",
      "Epoch [186/200], Loss: 1.0532, Accuracy: 0.5802\n",
      "Epoch [187/200], Loss: 1.0405, Accuracy: 0.5792\n",
      "Epoch [188/200], Loss: 1.0331, Accuracy: 0.5901\n",
      "Epoch [189/200], Loss: 1.0419, Accuracy: 0.5871\n",
      "Epoch [190/200], Loss: 1.0722, Accuracy: 0.5725\n",
      "Epoch [191/200], Loss: 1.0392, Accuracy: 0.5799\n",
      "Epoch [192/200], Loss: 1.0441, Accuracy: 0.5853\n",
      "Epoch [193/200], Loss: 1.0342, Accuracy: 0.5841\n",
      "Epoch [194/200], Loss: 1.0175, Accuracy: 0.5928\n",
      "Epoch [195/200], Loss: 1.0167, Accuracy: 0.5921\n",
      "Epoch [196/200], Loss: 1.0229, Accuracy: 0.5960\n",
      "Epoch [197/200], Loss: 1.0200, Accuracy: 0.5930\n",
      "Epoch [198/200], Loss: 1.0364, Accuracy: 0.5915\n",
      "Epoch [199/200], Loss: 1.0236, Accuracy: 0.5952\n",
      "Epoch [200/200], Loss: 1.0289, Accuracy: 0.5933\n"
     ]
    }
   ],
   "source": [
    "# LSTM with Attention\n",
    "lstm_atn_model = LSTMAttention(input_size[0], hidden_size, num_layers, num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(lstm_atn_model.parameters(), lr=0.001)\n",
    "train(lstm_atn_model, train_loaderLSTM, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Loss: 1.7136, Accuracy: 0.2466\n",
      "Epoch [2/200], Loss: 1.6368, Accuracy: 0.2886\n",
      "Epoch [3/200], Loss: 1.6027, Accuracy: 0.3183\n",
      "Epoch [4/200], Loss: 1.5774, Accuracy: 0.3178\n",
      "Epoch [5/200], Loss: 1.5693, Accuracy: 0.3266\n",
      "Epoch [6/200], Loss: 1.5528, Accuracy: 0.3370\n",
      "Epoch [7/200], Loss: 1.5429, Accuracy: 0.3430\n",
      "Epoch [8/200], Loss: 1.5430, Accuracy: 0.3375\n",
      "Epoch [9/200], Loss: 1.5307, Accuracy: 0.3425\n",
      "Epoch [10/200], Loss: 1.5228, Accuracy: 0.3465\n",
      "Epoch [11/200], Loss: 1.5088, Accuracy: 0.3581\n",
      "Epoch [12/200], Loss: 1.5125, Accuracy: 0.3555\n",
      "Epoch [13/200], Loss: 1.4992, Accuracy: 0.3585\n",
      "Epoch [14/200], Loss: 1.4991, Accuracy: 0.3670\n",
      "Epoch [15/200], Loss: 1.4940, Accuracy: 0.3632\n",
      "Epoch [16/200], Loss: 1.4888, Accuracy: 0.3660\n",
      "Epoch [17/200], Loss: 1.4817, Accuracy: 0.3724\n",
      "Epoch [18/200], Loss: 1.4830, Accuracy: 0.3741\n",
      "Epoch [19/200], Loss: 1.4649, Accuracy: 0.3766\n",
      "Epoch [20/200], Loss: 1.4689, Accuracy: 0.3822\n",
      "Epoch [21/200], Loss: 1.4617, Accuracy: 0.3828\n",
      "Epoch [22/200], Loss: 1.4572, Accuracy: 0.3924\n",
      "Epoch [23/200], Loss: 1.4533, Accuracy: 0.3902\n",
      "Epoch [24/200], Loss: 1.4596, Accuracy: 0.3790\n",
      "Epoch [25/200], Loss: 1.4526, Accuracy: 0.3961\n",
      "Epoch [26/200], Loss: 1.4410, Accuracy: 0.3924\n",
      "Epoch [27/200], Loss: 1.4364, Accuracy: 0.3980\n",
      "Epoch [28/200], Loss: 1.4252, Accuracy: 0.3953\n",
      "Epoch [29/200], Loss: 1.4223, Accuracy: 0.4010\n",
      "Epoch [30/200], Loss: 1.4254, Accuracy: 0.4016\n",
      "Epoch [31/200], Loss: 1.4192, Accuracy: 0.4010\n",
      "Epoch [32/200], Loss: 1.4228, Accuracy: 0.3969\n",
      "Epoch [33/200], Loss: 1.4087, Accuracy: 0.4111\n",
      "Epoch [34/200], Loss: 1.4079, Accuracy: 0.4117\n",
      "Epoch [35/200], Loss: 1.4153, Accuracy: 0.4067\n",
      "Epoch [36/200], Loss: 1.4081, Accuracy: 0.4142\n",
      "Epoch [37/200], Loss: 1.3890, Accuracy: 0.4242\n",
      "Epoch [38/200], Loss: 1.3939, Accuracy: 0.4211\n",
      "Epoch [39/200], Loss: 1.3910, Accuracy: 0.4206\n",
      "Epoch [40/200], Loss: 1.3939, Accuracy: 0.4277\n",
      "Epoch [41/200], Loss: 1.3858, Accuracy: 0.4297\n",
      "Epoch [42/200], Loss: 1.3787, Accuracy: 0.4272\n",
      "Epoch [43/200], Loss: 1.3863, Accuracy: 0.4200\n",
      "Epoch [44/200], Loss: 1.3875, Accuracy: 0.4196\n",
      "Epoch [45/200], Loss: 1.3849, Accuracy: 0.4322\n",
      "Epoch [46/200], Loss: 1.3791, Accuracy: 0.4327\n",
      "Epoch [47/200], Loss: 1.3775, Accuracy: 0.4305\n",
      "Epoch [48/200], Loss: 1.3589, Accuracy: 0.4421\n",
      "Epoch [49/200], Loss: 1.3734, Accuracy: 0.4331\n",
      "Epoch [50/200], Loss: 1.3553, Accuracy: 0.4356\n",
      "Epoch [51/200], Loss: 1.3537, Accuracy: 0.4415\n",
      "Epoch [52/200], Loss: 1.3644, Accuracy: 0.4420\n",
      "Epoch [53/200], Loss: 1.3496, Accuracy: 0.4413\n",
      "Epoch [54/200], Loss: 1.3548, Accuracy: 0.4391\n",
      "Epoch [55/200], Loss: 1.3475, Accuracy: 0.4502\n",
      "Epoch [56/200], Loss: 1.3436, Accuracy: 0.4420\n",
      "Epoch [57/200], Loss: 1.3521, Accuracy: 0.4445\n",
      "Epoch [58/200], Loss: 1.3465, Accuracy: 0.4413\n",
      "Epoch [59/200], Loss: 1.3454, Accuracy: 0.4468\n",
      "Epoch [60/200], Loss: 1.3287, Accuracy: 0.4468\n",
      "Epoch [61/200], Loss: 1.3379, Accuracy: 0.4487\n",
      "Epoch [62/200], Loss: 1.3372, Accuracy: 0.4539\n",
      "Epoch [63/200], Loss: 1.3365, Accuracy: 0.4492\n",
      "Epoch [64/200], Loss: 1.3368, Accuracy: 0.4463\n",
      "Epoch [65/200], Loss: 1.3272, Accuracy: 0.4552\n",
      "Epoch [66/200], Loss: 1.3337, Accuracy: 0.4576\n",
      "Epoch [67/200], Loss: 1.3207, Accuracy: 0.4628\n",
      "Epoch [68/200], Loss: 1.3225, Accuracy: 0.4566\n",
      "Epoch [69/200], Loss: 1.3224, Accuracy: 0.4618\n",
      "Epoch [70/200], Loss: 1.3351, Accuracy: 0.4460\n",
      "Epoch [71/200], Loss: 1.3324, Accuracy: 0.4554\n",
      "Epoch [72/200], Loss: 1.3305, Accuracy: 0.4583\n",
      "Epoch [73/200], Loss: 1.3180, Accuracy: 0.4614\n",
      "Epoch [74/200], Loss: 1.3164, Accuracy: 0.4559\n",
      "Epoch [75/200], Loss: 1.3206, Accuracy: 0.4519\n",
      "Epoch [76/200], Loss: 1.3160, Accuracy: 0.4598\n",
      "Epoch [77/200], Loss: 1.3012, Accuracy: 0.4683\n",
      "Epoch [78/200], Loss: 1.3082, Accuracy: 0.4621\n",
      "Epoch [79/200], Loss: 1.3041, Accuracy: 0.4636\n",
      "Epoch [80/200], Loss: 1.3188, Accuracy: 0.4536\n",
      "Epoch [81/200], Loss: 1.3036, Accuracy: 0.4628\n",
      "Epoch [82/200], Loss: 1.3027, Accuracy: 0.4626\n",
      "Epoch [83/200], Loss: 1.3132, Accuracy: 0.4700\n",
      "Epoch [84/200], Loss: 1.3241, Accuracy: 0.4530\n",
      "Epoch [85/200], Loss: 1.3112, Accuracy: 0.4651\n",
      "Epoch [86/200], Loss: 1.2937, Accuracy: 0.4732\n",
      "Epoch [87/200], Loss: 1.3215, Accuracy: 0.4618\n",
      "Epoch [88/200], Loss: 1.3005, Accuracy: 0.4630\n",
      "Epoch [89/200], Loss: 1.2924, Accuracy: 0.4698\n",
      "Epoch [90/200], Loss: 1.3138, Accuracy: 0.4662\n",
      "Epoch [91/200], Loss: 1.3039, Accuracy: 0.4668\n",
      "Epoch [92/200], Loss: 1.2977, Accuracy: 0.4692\n",
      "Epoch [93/200], Loss: 1.3010, Accuracy: 0.4707\n",
      "Epoch [94/200], Loss: 1.2925, Accuracy: 0.4688\n",
      "Epoch [95/200], Loss: 1.3356, Accuracy: 0.4547\n",
      "Epoch [96/200], Loss: 1.3108, Accuracy: 0.4633\n",
      "Epoch [97/200], Loss: 1.2887, Accuracy: 0.4739\n",
      "Epoch [98/200], Loss: 1.3063, Accuracy: 0.4675\n",
      "Epoch [99/200], Loss: 1.2954, Accuracy: 0.4705\n",
      "Epoch [100/200], Loss: 1.2874, Accuracy: 0.4730\n",
      "Epoch [101/200], Loss: 1.3056, Accuracy: 0.4719\n",
      "Epoch [102/200], Loss: 1.2967, Accuracy: 0.4774\n",
      "Epoch [103/200], Loss: 1.2881, Accuracy: 0.4700\n",
      "Epoch [104/200], Loss: 1.3027, Accuracy: 0.4702\n",
      "Epoch [105/200], Loss: 1.3170, Accuracy: 0.4559\n",
      "Epoch [106/200], Loss: 1.3008, Accuracy: 0.4633\n",
      "Epoch [107/200], Loss: 1.2744, Accuracy: 0.4774\n",
      "Epoch [108/200], Loss: 1.2814, Accuracy: 0.4766\n",
      "Epoch [109/200], Loss: 1.2834, Accuracy: 0.4714\n",
      "Epoch [110/200], Loss: 1.2769, Accuracy: 0.4767\n",
      "Epoch [111/200], Loss: 1.2848, Accuracy: 0.4690\n",
      "Epoch [112/200], Loss: 1.2736, Accuracy: 0.4850\n",
      "Epoch [113/200], Loss: 1.2771, Accuracy: 0.4776\n",
      "Epoch [114/200], Loss: 1.2894, Accuracy: 0.4678\n",
      "Epoch [115/200], Loss: 1.2854, Accuracy: 0.4727\n",
      "Epoch [116/200], Loss: 1.3117, Accuracy: 0.4667\n",
      "Epoch [117/200], Loss: 1.2894, Accuracy: 0.4782\n",
      "Epoch [118/200], Loss: 1.2916, Accuracy: 0.4709\n",
      "Epoch [119/200], Loss: 1.2704, Accuracy: 0.4804\n",
      "Epoch [120/200], Loss: 1.2780, Accuracy: 0.4771\n",
      "Epoch [121/200], Loss: 1.2762, Accuracy: 0.4734\n",
      "Epoch [122/200], Loss: 1.2738, Accuracy: 0.4804\n",
      "Epoch [123/200], Loss: 1.2737, Accuracy: 0.4851\n",
      "Epoch [124/200], Loss: 1.2699, Accuracy: 0.4794\n",
      "Epoch [125/200], Loss: 1.2556, Accuracy: 0.4855\n",
      "Epoch [126/200], Loss: 1.2808, Accuracy: 0.4777\n",
      "Epoch [127/200], Loss: 1.2801, Accuracy: 0.4695\n",
      "Epoch [128/200], Loss: 1.2609, Accuracy: 0.4838\n",
      "Epoch [129/200], Loss: 1.2652, Accuracy: 0.4863\n",
      "Epoch [130/200], Loss: 1.2717, Accuracy: 0.4781\n",
      "Epoch [131/200], Loss: 1.2641, Accuracy: 0.4853\n",
      "Epoch [132/200], Loss: 1.2752, Accuracy: 0.4845\n",
      "Epoch [133/200], Loss: 1.2812, Accuracy: 0.4821\n",
      "Epoch [134/200], Loss: 1.2629, Accuracy: 0.4870\n",
      "Epoch [135/200], Loss: 1.2707, Accuracy: 0.4794\n",
      "Epoch [136/200], Loss: 1.2601, Accuracy: 0.4846\n",
      "Epoch [137/200], Loss: 1.2920, Accuracy: 0.4744\n",
      "Epoch [138/200], Loss: 1.2735, Accuracy: 0.4866\n",
      "Epoch [139/200], Loss: 1.2726, Accuracy: 0.4855\n",
      "Epoch [140/200], Loss: 1.2646, Accuracy: 0.4861\n",
      "Epoch [141/200], Loss: 1.3009, Accuracy: 0.4655\n",
      "Epoch [142/200], Loss: 1.2493, Accuracy: 0.4895\n",
      "Epoch [143/200], Loss: 1.2665, Accuracy: 0.4828\n",
      "Epoch [144/200], Loss: 1.2559, Accuracy: 0.4912\n",
      "Epoch [145/200], Loss: 1.2724, Accuracy: 0.4769\n",
      "Epoch [146/200], Loss: 1.2579, Accuracy: 0.4845\n",
      "Epoch [147/200], Loss: 1.2788, Accuracy: 0.4806\n",
      "Epoch [148/200], Loss: 1.2430, Accuracy: 0.4922\n",
      "Epoch [149/200], Loss: 1.2548, Accuracy: 0.4836\n",
      "Epoch [150/200], Loss: 1.2576, Accuracy: 0.4851\n",
      "Epoch [151/200], Loss: 1.2569, Accuracy: 0.4838\n",
      "Epoch [152/200], Loss: 1.2710, Accuracy: 0.4835\n",
      "Epoch [153/200], Loss: 1.2739, Accuracy: 0.4793\n",
      "Epoch [154/200], Loss: 1.2642, Accuracy: 0.4811\n",
      "Epoch [155/200], Loss: 1.2639, Accuracy: 0.4861\n",
      "Epoch [156/200], Loss: 1.2615, Accuracy: 0.4868\n",
      "Epoch [157/200], Loss: 1.2594, Accuracy: 0.4831\n",
      "Epoch [158/200], Loss: 1.2450, Accuracy: 0.4997\n",
      "Epoch [159/200], Loss: 1.2557, Accuracy: 0.4897\n",
      "Epoch [160/200], Loss: 1.2547, Accuracy: 0.4895\n",
      "Epoch [161/200], Loss: 1.2630, Accuracy: 0.4853\n",
      "Epoch [162/200], Loss: 1.2549, Accuracy: 0.4922\n",
      "Epoch [163/200], Loss: 1.2389, Accuracy: 0.4866\n",
      "Epoch [164/200], Loss: 1.2376, Accuracy: 0.4967\n",
      "Epoch [165/200], Loss: 1.2444, Accuracy: 0.4950\n",
      "Epoch [166/200], Loss: 1.2841, Accuracy: 0.4709\n",
      "Epoch [167/200], Loss: 1.2451, Accuracy: 0.4883\n",
      "Epoch [168/200], Loss: 1.2469, Accuracy: 0.4860\n",
      "Epoch [169/200], Loss: 1.2580, Accuracy: 0.4883\n",
      "Epoch [170/200], Loss: 1.2475, Accuracy: 0.5004\n",
      "Epoch [171/200], Loss: 1.2630, Accuracy: 0.4799\n",
      "Epoch [172/200], Loss: 1.2556, Accuracy: 0.4942\n",
      "Epoch [173/200], Loss: 1.2739, Accuracy: 0.4840\n",
      "Epoch [174/200], Loss: 1.2680, Accuracy: 0.4809\n",
      "Epoch [175/200], Loss: 1.2529, Accuracy: 0.4885\n",
      "Epoch [176/200], Loss: 1.2738, Accuracy: 0.4829\n",
      "Epoch [177/200], Loss: 1.2605, Accuracy: 0.4873\n",
      "Epoch [178/200], Loss: 1.2568, Accuracy: 0.4919\n",
      "Epoch [179/200], Loss: 1.2663, Accuracy: 0.4813\n",
      "Epoch [180/200], Loss: 1.2295, Accuracy: 0.5003\n",
      "Epoch [181/200], Loss: 1.2603, Accuracy: 0.4846\n",
      "Epoch [182/200], Loss: 1.2569, Accuracy: 0.4804\n",
      "Epoch [183/200], Loss: 1.2459, Accuracy: 0.4972\n",
      "Epoch [184/200], Loss: 1.2402, Accuracy: 0.4927\n",
      "Epoch [185/200], Loss: 1.2242, Accuracy: 0.4996\n",
      "Epoch [186/200], Loss: 1.2565, Accuracy: 0.4925\n",
      "Epoch [187/200], Loss: 1.2405, Accuracy: 0.4947\n",
      "Epoch [188/200], Loss: 1.2563, Accuracy: 0.4964\n",
      "Epoch [189/200], Loss: 1.2310, Accuracy: 0.5014\n",
      "Epoch [190/200], Loss: 1.2436, Accuracy: 0.4991\n",
      "Epoch [191/200], Loss: 1.2424, Accuracy: 0.5009\n",
      "Epoch [192/200], Loss: 1.2537, Accuracy: 0.4954\n",
      "Epoch [193/200], Loss: 1.2579, Accuracy: 0.4895\n",
      "Epoch [194/200], Loss: 1.2426, Accuracy: 0.4908\n",
      "Epoch [195/200], Loss: 1.2497, Accuracy: 0.4868\n",
      "Epoch [196/200], Loss: 1.2422, Accuracy: 0.4877\n",
      "Epoch [197/200], Loss: 1.2937, Accuracy: 0.4762\n",
      "Epoch [198/200], Loss: 1.2493, Accuracy: 0.4910\n",
      "Epoch [199/200], Loss: 1.2439, Accuracy: 0.4940\n",
      "Epoch [200/200], Loss: 1.2384, Accuracy: 0.4997\n"
     ]
    }
   ],
   "source": [
    "# Convolutional Neural Network\n",
    "cnn_model = CNNModel(input_size[0],num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)\n",
    "train(cnn_model, train_loaderCNN, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Loss: 1.7601, Accuracy: 0.2703\n",
      "Epoch [2/200], Loss: 1.5680, Accuracy: 0.3387\n",
      "Epoch [3/200], Loss: 1.5330, Accuracy: 0.3578\n",
      "Epoch [4/200], Loss: 1.5090, Accuracy: 0.3677\n",
      "Epoch [5/200], Loss: 1.4774, Accuracy: 0.3837\n",
      "Epoch [6/200], Loss: 1.4736, Accuracy: 0.3968\n",
      "Epoch [7/200], Loss: 1.4563, Accuracy: 0.4027\n",
      "Epoch [8/200], Loss: 1.4333, Accuracy: 0.4111\n",
      "Epoch [9/200], Loss: 1.4334, Accuracy: 0.4099\n",
      "Epoch [10/200], Loss: 1.4410, Accuracy: 0.4060\n",
      "Epoch [11/200], Loss: 1.4092, Accuracy: 0.4189\n",
      "Epoch [12/200], Loss: 1.4149, Accuracy: 0.4176\n",
      "Epoch [13/200], Loss: 1.4037, Accuracy: 0.4242\n",
      "Epoch [14/200], Loss: 1.3975, Accuracy: 0.4258\n",
      "Epoch [15/200], Loss: 1.4002, Accuracy: 0.4287\n",
      "Epoch [16/200], Loss: 1.3883, Accuracy: 0.4319\n",
      "Epoch [17/200], Loss: 1.3626, Accuracy: 0.4420\n",
      "Epoch [18/200], Loss: 1.3644, Accuracy: 0.4406\n",
      "Epoch [19/200], Loss: 1.3606, Accuracy: 0.4465\n",
      "Epoch [20/200], Loss: 1.3532, Accuracy: 0.4425\n",
      "Epoch [21/200], Loss: 1.3344, Accuracy: 0.4546\n",
      "Epoch [22/200], Loss: 1.3365, Accuracy: 0.4522\n",
      "Epoch [23/200], Loss: 1.3362, Accuracy: 0.4638\n",
      "Epoch [24/200], Loss: 1.3639, Accuracy: 0.4420\n",
      "Epoch [25/200], Loss: 1.3292, Accuracy: 0.4598\n",
      "Epoch [26/200], Loss: 1.3424, Accuracy: 0.4583\n",
      "Epoch [27/200], Loss: 1.3201, Accuracy: 0.4601\n",
      "Epoch [28/200], Loss: 1.3008, Accuracy: 0.4740\n",
      "Epoch [29/200], Loss: 1.3082, Accuracy: 0.4682\n",
      "Epoch [30/200], Loss: 1.2987, Accuracy: 0.4670\n",
      "Epoch [31/200], Loss: 1.2890, Accuracy: 0.4777\n",
      "Epoch [32/200], Loss: 1.2990, Accuracy: 0.4818\n",
      "Epoch [33/200], Loss: 1.2869, Accuracy: 0.4813\n",
      "Epoch [34/200], Loss: 1.2942, Accuracy: 0.4816\n",
      "Epoch [35/200], Loss: 1.2896, Accuracy: 0.4828\n",
      "Epoch [36/200], Loss: 1.2737, Accuracy: 0.4846\n",
      "Epoch [37/200], Loss: 1.2662, Accuracy: 0.4934\n",
      "Epoch [38/200], Loss: 1.2495, Accuracy: 0.4991\n",
      "Epoch [39/200], Loss: 1.2492, Accuracy: 0.4991\n",
      "Epoch [40/200], Loss: 1.2495, Accuracy: 0.5026\n",
      "Epoch [41/200], Loss: 1.2362, Accuracy: 0.5068\n",
      "Epoch [42/200], Loss: 1.2452, Accuracy: 0.5073\n",
      "Epoch [43/200], Loss: 1.2574, Accuracy: 0.4893\n",
      "Epoch [44/200], Loss: 1.2278, Accuracy: 0.5102\n",
      "Epoch [45/200], Loss: 1.2437, Accuracy: 0.4981\n",
      "Epoch [46/200], Loss: 1.2158, Accuracy: 0.5103\n",
      "Epoch [47/200], Loss: 1.2138, Accuracy: 0.5139\n",
      "Epoch [48/200], Loss: 1.2215, Accuracy: 0.5081\n",
      "Epoch [49/200], Loss: 1.2406, Accuracy: 0.5008\n",
      "Epoch [50/200], Loss: 1.2085, Accuracy: 0.5090\n",
      "Epoch [51/200], Loss: 1.1993, Accuracy: 0.5249\n",
      "Epoch [52/200], Loss: 1.2437, Accuracy: 0.5014\n",
      "Epoch [53/200], Loss: 1.2135, Accuracy: 0.5130\n",
      "Epoch [54/200], Loss: 1.2036, Accuracy: 0.5135\n",
      "Epoch [55/200], Loss: 1.1875, Accuracy: 0.5286\n",
      "Epoch [56/200], Loss: 1.2117, Accuracy: 0.5105\n",
      "Epoch [57/200], Loss: 1.1907, Accuracy: 0.5221\n",
      "Epoch [58/200], Loss: 1.1852, Accuracy: 0.5234\n",
      "Epoch [59/200], Loss: 1.1740, Accuracy: 0.5244\n",
      "Epoch [60/200], Loss: 1.1692, Accuracy: 0.5256\n",
      "Epoch [61/200], Loss: 1.2009, Accuracy: 0.5125\n",
      "Epoch [62/200], Loss: 1.1737, Accuracy: 0.5317\n",
      "Epoch [63/200], Loss: 1.1656, Accuracy: 0.5333\n",
      "Epoch [64/200], Loss: 1.1585, Accuracy: 0.5365\n",
      "Epoch [65/200], Loss: 1.1592, Accuracy: 0.5377\n",
      "Epoch [66/200], Loss: 1.1635, Accuracy: 0.5333\n",
      "Epoch [67/200], Loss: 1.1449, Accuracy: 0.5429\n",
      "Epoch [68/200], Loss: 1.1470, Accuracy: 0.5461\n",
      "Epoch [69/200], Loss: 1.1550, Accuracy: 0.5407\n",
      "Epoch [70/200], Loss: 1.1463, Accuracy: 0.5451\n",
      "Epoch [71/200], Loss: 1.1430, Accuracy: 0.5461\n",
      "Epoch [72/200], Loss: 1.1891, Accuracy: 0.5243\n",
      "Epoch [73/200], Loss: 1.1290, Accuracy: 0.5570\n",
      "Epoch [74/200], Loss: 1.1300, Accuracy: 0.5493\n",
      "Epoch [75/200], Loss: 1.1251, Accuracy: 0.5496\n",
      "Epoch [76/200], Loss: 1.1222, Accuracy: 0.5584\n",
      "Epoch [77/200], Loss: 1.1332, Accuracy: 0.5508\n",
      "Epoch [78/200], Loss: 1.1197, Accuracy: 0.5564\n",
      "Epoch [79/200], Loss: 1.1188, Accuracy: 0.5570\n",
      "Epoch [80/200], Loss: 1.1062, Accuracy: 0.5589\n",
      "Epoch [81/200], Loss: 1.1036, Accuracy: 0.5659\n",
      "Epoch [82/200], Loss: 1.1066, Accuracy: 0.5663\n",
      "Epoch [83/200], Loss: 1.1181, Accuracy: 0.5538\n",
      "Epoch [84/200], Loss: 1.1292, Accuracy: 0.5562\n",
      "Epoch [85/200], Loss: 1.1228, Accuracy: 0.5597\n",
      "Epoch [86/200], Loss: 1.1108, Accuracy: 0.5552\n",
      "Epoch [87/200], Loss: 1.1089, Accuracy: 0.5592\n",
      "Epoch [88/200], Loss: 1.0994, Accuracy: 0.5622\n",
      "Epoch [89/200], Loss: 1.1049, Accuracy: 0.5609\n",
      "Epoch [90/200], Loss: 1.1024, Accuracy: 0.5585\n",
      "Epoch [91/200], Loss: 1.0869, Accuracy: 0.5671\n",
      "Epoch [92/200], Loss: 1.0967, Accuracy: 0.5743\n",
      "Epoch [93/200], Loss: 1.0963, Accuracy: 0.5658\n",
      "Epoch [94/200], Loss: 1.0879, Accuracy: 0.5740\n",
      "Epoch [95/200], Loss: 1.0806, Accuracy: 0.5674\n",
      "Epoch [96/200], Loss: 1.0721, Accuracy: 0.5743\n",
      "Epoch [97/200], Loss: 1.0696, Accuracy: 0.5790\n",
      "Epoch [98/200], Loss: 1.0656, Accuracy: 0.5777\n",
      "Epoch [99/200], Loss: 1.0902, Accuracy: 0.5706\n",
      "Epoch [100/200], Loss: 1.0774, Accuracy: 0.5757\n",
      "Epoch [101/200], Loss: 1.0647, Accuracy: 0.5753\n",
      "Epoch [102/200], Loss: 1.0744, Accuracy: 0.5701\n",
      "Epoch [103/200], Loss: 1.0841, Accuracy: 0.5790\n",
      "Epoch [104/200], Loss: 1.0562, Accuracy: 0.5792\n",
      "Epoch [105/200], Loss: 1.0600, Accuracy: 0.5777\n",
      "Epoch [106/200], Loss: 1.0637, Accuracy: 0.5769\n",
      "Epoch [107/200], Loss: 1.0542, Accuracy: 0.5832\n",
      "Epoch [108/200], Loss: 1.0425, Accuracy: 0.5861\n",
      "Epoch [109/200], Loss: 1.0715, Accuracy: 0.5794\n",
      "Epoch [110/200], Loss: 1.0800, Accuracy: 0.5693\n",
      "Epoch [111/200], Loss: 1.0486, Accuracy: 0.5821\n",
      "Epoch [112/200], Loss: 1.0479, Accuracy: 0.5794\n",
      "Epoch [113/200], Loss: 1.0355, Accuracy: 0.5896\n",
      "Epoch [114/200], Loss: 1.0346, Accuracy: 0.5893\n",
      "Epoch [115/200], Loss: 1.0296, Accuracy: 0.5940\n",
      "Epoch [116/200], Loss: 1.0249, Accuracy: 0.5960\n",
      "Epoch [117/200], Loss: 1.0498, Accuracy: 0.5884\n",
      "Epoch [118/200], Loss: 1.0464, Accuracy: 0.5856\n",
      "Epoch [119/200], Loss: 1.0342, Accuracy: 0.5926\n",
      "Epoch [120/200], Loss: 1.0289, Accuracy: 0.5955\n",
      "Epoch [121/200], Loss: 1.0256, Accuracy: 0.5908\n",
      "Epoch [122/200], Loss: 1.0291, Accuracy: 0.5952\n",
      "Epoch [123/200], Loss: 1.0270, Accuracy: 0.5878\n",
      "Epoch [124/200], Loss: 1.0454, Accuracy: 0.5901\n",
      "Epoch [125/200], Loss: 1.0456, Accuracy: 0.5893\n",
      "Epoch [126/200], Loss: 1.0277, Accuracy: 0.5935\n",
      "Epoch [127/200], Loss: 1.0647, Accuracy: 0.5834\n",
      "Epoch [128/200], Loss: 1.0288, Accuracy: 0.6014\n",
      "Epoch [129/200], Loss: 1.0487, Accuracy: 0.5916\n",
      "Epoch [130/200], Loss: 1.0187, Accuracy: 0.5967\n",
      "Epoch [131/200], Loss: 1.0184, Accuracy: 0.5967\n",
      "Epoch [132/200], Loss: 1.0106, Accuracy: 0.6069\n",
      "Epoch [133/200], Loss: 1.0172, Accuracy: 0.5963\n",
      "Epoch [134/200], Loss: 1.0094, Accuracy: 0.6042\n",
      "Epoch [135/200], Loss: 1.0047, Accuracy: 0.6007\n",
      "Epoch [136/200], Loss: 1.0180, Accuracy: 0.5977\n",
      "Epoch [137/200], Loss: 1.0035, Accuracy: 0.5997\n",
      "Epoch [138/200], Loss: 1.0155, Accuracy: 0.6002\n",
      "Epoch [139/200], Loss: 1.0090, Accuracy: 0.6071\n",
      "Epoch [140/200], Loss: 1.0089, Accuracy: 0.6010\n",
      "Epoch [141/200], Loss: 1.0113, Accuracy: 0.5994\n",
      "Epoch [142/200], Loss: 0.9997, Accuracy: 0.6039\n",
      "Epoch [143/200], Loss: 1.0048, Accuracy: 0.6064\n",
      "Epoch [144/200], Loss: 1.0070, Accuracy: 0.5938\n",
      "Epoch [145/200], Loss: 0.9927, Accuracy: 0.6027\n",
      "Epoch [146/200], Loss: 0.9909, Accuracy: 0.6101\n",
      "Epoch [147/200], Loss: 0.9862, Accuracy: 0.6155\n",
      "Epoch [148/200], Loss: 1.0039, Accuracy: 0.6118\n",
      "Epoch [149/200], Loss: 0.9960, Accuracy: 0.6062\n",
      "Epoch [150/200], Loss: 0.9920, Accuracy: 0.6094\n",
      "Epoch [151/200], Loss: 1.0205, Accuracy: 0.5918\n",
      "Epoch [152/200], Loss: 0.9817, Accuracy: 0.6104\n",
      "Epoch [153/200], Loss: 0.9753, Accuracy: 0.6160\n",
      "Epoch [154/200], Loss: 1.0023, Accuracy: 0.6004\n",
      "Epoch [155/200], Loss: 1.0304, Accuracy: 0.5905\n",
      "Epoch [156/200], Loss: 0.9908, Accuracy: 0.6110\n",
      "Epoch [157/200], Loss: 0.9877, Accuracy: 0.6113\n",
      "Epoch [158/200], Loss: 0.9787, Accuracy: 0.6111\n",
      "Epoch [159/200], Loss: 0.9952, Accuracy: 0.6088\n",
      "Epoch [160/200], Loss: 1.0270, Accuracy: 0.5978\n",
      "Epoch [161/200], Loss: 0.9952, Accuracy: 0.6141\n",
      "Epoch [162/200], Loss: 0.9781, Accuracy: 0.6182\n",
      "Epoch [163/200], Loss: 0.9888, Accuracy: 0.6110\n",
      "Epoch [164/200], Loss: 0.9677, Accuracy: 0.6227\n",
      "Epoch [165/200], Loss: 0.9708, Accuracy: 0.6140\n",
      "Epoch [166/200], Loss: 0.9755, Accuracy: 0.6155\n",
      "Epoch [167/200], Loss: 0.9998, Accuracy: 0.6078\n",
      "Epoch [168/200], Loss: 0.9958, Accuracy: 0.6110\n",
      "Epoch [169/200], Loss: 0.9835, Accuracy: 0.6086\n",
      "Epoch [170/200], Loss: 0.9783, Accuracy: 0.6158\n",
      "Epoch [171/200], Loss: 0.9702, Accuracy: 0.6155\n",
      "Epoch [172/200], Loss: 0.9660, Accuracy: 0.6148\n",
      "Epoch [173/200], Loss: 0.9619, Accuracy: 0.6227\n",
      "Epoch [174/200], Loss: 0.9689, Accuracy: 0.6190\n",
      "Epoch [175/200], Loss: 0.9776, Accuracy: 0.6121\n",
      "Epoch [176/200], Loss: 0.9742, Accuracy: 0.6128\n",
      "Epoch [177/200], Loss: 0.9601, Accuracy: 0.6267\n",
      "Epoch [178/200], Loss: 0.9579, Accuracy: 0.6311\n",
      "Epoch [179/200], Loss: 0.9792, Accuracy: 0.6200\n",
      "Epoch [180/200], Loss: 0.9717, Accuracy: 0.6194\n",
      "Epoch [181/200], Loss: 0.9623, Accuracy: 0.6170\n",
      "Epoch [182/200], Loss: 0.9628, Accuracy: 0.6168\n",
      "Epoch [183/200], Loss: 0.9607, Accuracy: 0.6207\n",
      "Epoch [184/200], Loss: 0.9610, Accuracy: 0.6210\n",
      "Epoch [185/200], Loss: 0.9560, Accuracy: 0.6267\n",
      "Epoch [186/200], Loss: 0.9476, Accuracy: 0.6278\n",
      "Epoch [187/200], Loss: 0.9615, Accuracy: 0.6214\n",
      "Epoch [188/200], Loss: 1.0036, Accuracy: 0.6042\n",
      "Epoch [189/200], Loss: 0.9488, Accuracy: 0.6281\n",
      "Epoch [190/200], Loss: 0.9667, Accuracy: 0.6197\n",
      "Epoch [191/200], Loss: 0.9522, Accuracy: 0.6272\n",
      "Epoch [192/200], Loss: 0.9341, Accuracy: 0.6345\n",
      "Epoch [193/200], Loss: 0.9583, Accuracy: 0.6289\n",
      "Epoch [194/200], Loss: 0.9626, Accuracy: 0.6170\n",
      "Epoch [195/200], Loss: 0.9575, Accuracy: 0.6254\n",
      "Epoch [196/200], Loss: 0.9588, Accuracy: 0.6236\n",
      "Epoch [197/200], Loss: 0.9429, Accuracy: 0.6244\n",
      "Epoch [198/200], Loss: 0.9520, Accuracy: 0.6328\n",
      "Epoch [199/200], Loss: 0.9363, Accuracy: 0.6286\n",
      "Epoch [200/200], Loss: 0.9437, Accuracy: 0.6321\n"
     ]
    }
   ],
   "source": [
    "# Convolutional Neural Network with Attention\n",
    "cnn_atn_model = CNNAttention(input_size[0],num_classes).to(device)\n",
    "optimizer = optim.Adam(cnn_atn_model.parameters(), lr=0.001)\n",
    "train(cnn_atn_model, train_loaderCNN, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para teste\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, torch.max(labels, 1)[1])\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == torch.max(labels, 1)[1]).sum().item()\n",
    "\n",
    "    test_loss = running_loss / len(test_loader)\n",
    "    test_accuracy = correct / total\n",
    "    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing SimpleDNN:\n",
      "Test Loss: 1.4349, Test Accuracy: 0.4291\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing SimpleDNN:\")\n",
    "test(sdnn_model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing LSTMModel:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.6032, Test Accuracy: 0.4305\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTesting LSTMModel:\")\n",
    "test(lstm_model, test_loaderLSTM, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing LSTMAttention:\n",
      "Test Loss: 1.7107, Test Accuracy: 0.4338\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTesting LSTMAttention:\")\n",
    "test(lstm_atn_model, test_loaderLSTM, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing CNNModel:\n",
      "Test Loss: 1.4820, Test Accuracy: 0.3875\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTesting CNNModel:\")\n",
    "test(cnn_model, test_loaderCNN, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing CNNAttention:\n",
      "Test Loss: 1.8222, Test Accuracy: 0.4090\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTesting CNNAttention:\")\n",
    "test(cnn_atn_model, test_loaderCNN, criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
